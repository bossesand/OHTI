<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1 plus MathML 2.0//EN" "http://www.w3.org/Math/DTD/mathml2/xhtml-math11-f.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><!--This file was converted to xhtml by LibreOffice - see http://cgit.freedesktop.org/libreoffice/core/tree/filter/source/xslt for the code.--><head profile="http://dublincore.org/documents/dcmi-terms/"><meta http-equiv="Content-Type" content="application/xhtml+xml; charset=utf-8"/><title xml:lang="en-US">Source:</title><meta name="DCTERMS.title" content="Source:" xml:lang="en-US"/><meta name="DCTERMS.language" content="en-US" scheme="DCTERMS.RFC4646"/><meta name="DCTERMS.source" content="http://xml.openoffice.org/odf2xhtml"/><meta name="DCTERMS.creator" content="Stefan Schreiber"/><meta name="DCTERMS.issued" content="2018-05-16T02:42:00" scheme="DCTERMS.W3CDTF"/><meta name="DCTERMS.contributor" content="Stefan Schreiber"/><meta name="DCTERMS.modified" content="2018-05-16T02:42:00" scheme="DCTERMS.W3CDTF"/><meta name="DCTERMS.provenance" content="" xml:lang="en-US"/><meta name="DCTERMS.subject" content="," xml:lang="en-US"/><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/" hreflang="en"/><link rel="schema.DCTERMS" href="http://purl.org/dc/terms/" hreflang="en"/><link rel="schema.DCTYPE" href="http://purl.org/dc/dcmitype/" hreflang="en"/><link rel="schema.DCAM" href="http://purl.org/dc/dcam/" hreflang="en"/><style type="text/css">
	@page {  }
	table { border-collapse:collapse; border-spacing:0; empty-cells:show }
	td, th { vertical-align:top; font-size:12pt;}
	h1, h2, h3, h4, h5, h6 { clear:both }
	ol, ul { margin:0; padding:0;}
	li { list-style: none; margin:0; padding:0;}
	<!-- "li span.odfLiEnd" - IE 7 issue-->
	li span. { clear: both; line-height:0; width:0; height:0; margin:0; padding:0; }
	span.footnodeNumber { padding-right:1em; }
	span.annotation_style_by_filter { font-size:95%; font-family:Arial; background-color:#fff000;  margin:0; border:0; padding:0;  }
	* { margin:0;}
	.P1 { font-size:12pt; font-family:Times New Roman; writing-mode:lr-tb; }
	.P2 { font-size:12pt; font-family:Times New Roman; writing-mode:lr-tb; text-align:center ! important; }
	.P3 { font-size:12pt; font-family:Times New Roman; writing-mode:lr-tb; line-height:150%; }
	.P4 { font-size:12pt; font-family:Times New Roman; writing-mode:lr-tb; }
	.P5 { font-size:12pt; text-align:center ! important; font-family:Times New Roman; writing-mode:lr-tb; line-height:150%; }
	.P6 { font-size:12pt; margin-bottom:0.106cm; margin-top:0.423cm; font-family:Arial; writing-mode:lr-tb; text-align:center ! important; }
	.Text_20_body { font-size:12pt; font-family:Times New Roman; writing-mode:lr-tb; }
	<!-- ODF styles with no properties representable as CSS -->
	 { }
	</style></head><body dir="ltr" style="max-width:21.001cm;margin-top:2.499cm; margin-bottom:2.499cm; margin-left:3cm; margin-right:3cm; writing-mode:lr-tb; "><p class="P4">Source:</p><p class="P1">Open Headtracking Initiative for Audio (OHTI)</p><p class="P1">Bo-Erik Sandholm, Stockholm</p><p class="P1">Roger Sandholm, Stockholm</p><p class="P1">Stefan Schreiber, Lisbon</p><p class="P1"> </p><p class="Text_20_body">Subject/Area: Presentation of OHTI; Discussion of current results and project state, and outlook on the planned features for the next version</p><p class="P2">(Introduction, written for audio experts)</p><p class="P1"> </p><p class="P1"> </p><h3 class="P6"><a id="a__On_Open_Audio_Headtracking"><span/></a>On Open Audio Headtracking</h3><p class="P1">(involving an open audio headtracker, a proposal to standardize audio headtrackers with help of a common API, personalisation of binaural playback software - and in the future measures to provide BRIR and room synthesis)</p><p class="P1"> </p><p class="P3">This project has a quite long history, starting on a theoretical level in 2014 if not earlier.</p><p class="P3">As it is well-known binaural audio (so audio reproduced via headphones) usually suffers from certain problems and shortcomings. Listening to stereo recordings on headphones works because of the available spatial ITD and ILD cues, which are sufficient to deliver a convincing stereo impression. However, because of the lack of any cues indicating that the sound would come from outside (filter effects of the body, torso and pinnae) in the source to earphone reproduction chain, the stereo effect will be perceived as in-head stereo. So even if listening to stereo recordings via headphones and earphones “kind of” works: certain spatial cues are just missing, and this method is not able to reproduce a natural listening experience. (As consequence often experience of listening fatigue, after longer exposure to headphones.)</p><p class="P3">Some specific recordings aimed for binaural listening (usually recorded by “kunstkopf” type microphones) would deliver head- and pinnae-related filter cues, but not in a personalized way. There is also no way to rotate these recordings according to your head and body movements.</p><p class="P3"> </p><p class="P3">Speaking about surround sound, the naive headphone approach based on ITD/ILD differences just can’t work.  You will need a filter-based approach to reproduce surround sound via headphones, in any case. (Ideally these filters match the individual head shadows, earshapes etc.)</p><p class="P3"> </p><p class="P3">To get around these limitations you can decode available audio sources and music recordings via dedicated binaural playback software, applying some form of HRTF (Head-related Transfer Function)  or BRIR (Binaural Room Impulse Response) filters. In both cases it is advantageous if headtracking is also applied, because it is known that small head movements are part of the natural hearing process and are providing useful spatial cues -  even if the listener is not moving his body. (In the context of VR Audio it is obvious that the audio field has to be adapted according to the body and head movements.)</p><p class="P3"> </p><p class="P3">Whereas audio headtracking is done in research since decades and is commercially applied by a few CE headphones and receiver-type high-end solutions (aimed for headphone monitoring and HiFi applications), it would be a good idea to &lt; standardize &gt; the access to audio headtrackers - allowing to apply standardized and general playback software, which would not have to be adapted to any specific motion sensor. (In the end any motion sensor provides common data types such as acceleration, orientation etc. The sensor data types and measurement readings are what matter, from a systems perspective.)</p><p class="P3"> </p><p class="P3">Co-representing a small team which works (in part-time) since about 2015 on an API based standard for open headtracking called OHTI - and on some corresponding open reference hardware and software solutions to build on and to showcase the viability of the concept - I take the the opportunity to inform a wider and general audience about this project and its current state.</p><p class="P3"> </p><p class="P3">(A slightly different but similar version of this text has recently been sent to a well-known standard organisation, as continuation of proposals in 2014 and 2017.)</p><p class="P3"> </p><p class="P3"> </p><p class="P3">A short presentation of the OHTI project:</p><p class="P3"> </p><p class="P3">1. We (Open Headtracking Initiative/OHTI) have - according to my best knowledge - realized the first fully open BLE (Bluetooth Low Energy) based audio headtracker, which is shown to be functional in combination with our versions of Google Omnitone and JSAmbisonics. (The OHTI reference sensor also can communicate via USB.)</p><p class="P3"/><p class="P3">The available open hardware and software ressources will serve as reference solution to implement an open standard API for 3DoF (orientation), and maybe later a 6DoF (pose) sensor API. The aim is to unify the logical interface for motion sensors, and to be able to offer sensor-independent playback software for headphones of all types.</p><p class="P3"> </p><p class="P3">(That such an interface must be possible is clear if you study the sensor APIs of certain OSs, such as Android and iOS: The equivalent measurement data of different sensors are accessed via the same function calls. The only difference to OHTI is that in the OHTI case motion/orientation chips are &lt; external &gt;, not some internal part of a computer.)</p><p class="P3"> </p><p class="P3">6DoF is an easy extension of the OHTI interface, seen from a standardization and API perspective. (More about the planned 3DoF/6DoF sensor APIs in Annex A.)</p><p class="P3"> </p><p class="P3">Our hardware description and software sources are already available at https://github.com/bossesand/ohti, currently at alpha stage but already functional. </p><p class="P3"> </p><p class="P3">Hardware description:</p><p class="P3">The reference implementation is based on chips which should be performant, relatively cheap, easily available/sourceable, and reliable. Our current reference motion sensor is the BNO055 from Bosch Sensortec.</p><p class="P3">(This chip has been proven to be a good choice also by a team from IEM Graz, presenting a similar project in 2017 at the AES conference in Berlin:</p><p class="P3">https://www.researchgate.net/publication/315797712_Implementation_and_Evaluation_of_a_Low-cost_Head-tracker_for_Binaural_Synthesis</p><p class="P3"> </p><p class="P3">Compared to this nice but relatively limited implementation - which is USB-based only - we also add BLE support, support of hrtf personalisation, and the intention to standardize at least a common headtracking/3DoF API, if not also an 6DoF API.)</p><p class="P3"> </p><p class="P3">Our Bluetooth Low Energy (BLE) hardware is based on the nrf52832 SoC, offered by Nordic Semiconductor.</p><p class="P3">(This chip was a good choice back in 2016, in the sense that it could and has been updated to support BT5 later:</p><p class="P3">https://www.nordicsemi.com/eng/Products/Bluetooth-low-energy/nRF52832</p><p class="P3">)</p><p class="P3"> </p><p class="P3">Consequently the OHTI reference will work together with BLE 4.0 and above components and equipment, but can be upgraded to BT5 if and when needed.  A practical advantage of BT5 would be lower latency, for example. Currently most “legacy” smartphones, tablets and PCs don’t support BT5 yet, so this is for a future upgrade.</p><p class="P3"> </p><p class="P3">The specific board used for our OHTI prototype implementation is the Adafruit Feather nRF52 Bluefruit LE, which is more or less Arduino-compatible and offers USB and BLE interfaces. </p><p class="P3">A commercial headtracking module solution should still be a bit more compact, but could be based on quite similar hardware - and the same “main chip” combination of nrf52832 and BNO055.</p><p class="P3"> </p><p class="P3">The headtracker is connected to some OHTI middleware (written in JS/node.js) on the host (PC, tablet, smartphone) side, which currently will connect to two Ambisonics players written in JS/HTML5.</p><p class="P3"> </p><p class="P3">Our software currently seems to work on all major desktop operating systems. Adaptations to Android and iOS should be possible. (We didn’t implement “mobile” versions yet, because desktop OSs are a  better environment for sw development.)</p><p class="P3">The implemented software solutions are (mainly) written in JavaScript, and designed to be (nearly) platform-independent.</p><p class="P3"> </p><p class="P3">Headtracked playback of Ambisonics up to third order is showcased via our derivations of Google Omnitone (OHTI-Omnitone) and JSAmbisonics (a similar JS library and player, written by Archontis Politis and David Poirier-Quinot), as part of our current reference solution.</p><p class="P3"> </p><p class="P3">We also have realized and demonstrated a complete chain of hrtf personalisation, in the sense that a personalized Sofa file of my partner Bo-Erik (provided via Finnish startup company IDA Audio, using a user-provided video of the head/pinnae to - in my understanding - construct a 3D head model derived from optical data, and to calculate some Hrtf data set via the BEM method) has been used to personalize both our binaural ambisonics player variants - with success.</p><p class="P3"> </p><p class="P3">In the Omnitone implementation personalization is shown only for 1st order Ambisonics (personalization of the hrir files for Omnitone’s virtual speaker decoder, using a cube configuration), whereas our JSAmbisonics implementation (OHTI-JSAmbisonics) already supports headtracking &lt; and  &gt;  personalisation for up to 3rd order Ambisonics. (HOA orders &gt; =  4 could be supported, in principle.)</p><p class="P3"> </p><p class="P3">Our reference implementation is able to switch between both players from a gui controlled (test) environment, and to dynamically load personalized hrtf files into the JSAmbisonics player. AES-69/SOFA files currently still have to be converted to JSAmbisonics’ internal JSON-based data format before. However,  to implement direct and dynamic SOFA file support in OHTI-JSAmbisonics would be just a relatively minor and feasible update. </p><p class="P3"> </p><p class="P3">The update rate of our headtracking solution is currently set to about 80Hz, which could be extended to 100Hz - the design limit of the BNO055 chip. The latency  of our software solution seems to be unnoticeable - at least for audio-only solutions. Good latency performance (i.e. low latency times of about &lt;= 40ms) for AV applications including TV audio would still  have to be verified. Pure VR applications would be based on HMDs and their respective tracking solutions. (HMDs and their sensors could be unified via sensor APIs similar to OHTI and flexible video interfaces, resolving resolution interpolation and so on.)</p><p class="P3"> </p><p class="P3">I suspect that the latency performance of our software could still be improved, because we are still at alpha stage  and didn’t really optimize performance yet. (Performance seems to be quite good anyway. You are invited to verify...)</p><p class="P3"> </p><p class="P3"> </p><p class="P3">2. After the presentation of the current state of the OHTI project, I will give an outlook on some steps we plan to implement within OHTI in some hopefully not too distant future.</p><p class="P3"> </p><p class="P3">I. HRTF personalisation (via the AES-69/SOFA standard) will certainly be supported by OHTI, in general and (planned) for all forms of supported audio content.</p><p class="P3"> </p><p class="P3">However, we (OHTI team) are noticing that HRTF personalisation is probably not “enough” or even adequate to reproduce channel-based audio content, which would be relevant for the reproduction of most existing music recordings - but could be relevant for VR Audio as well. (Details not discussed in this context.)</p><p class="P3"> </p><p class="P3">Binaural reproduction of channel-based audio would normally require BRIR (Binaural Room Impulse Response) data sets, which can be seen as “long” HRIR data sets including the head/body filters, but also the (hrtf filtered) room impulse responses.</p><p class="P3">(To compare: The Mpeg 3DA television-audio system is using LR-BRIR data sets for binaural reproduction of channel-based audio content. However, these LR-BRIR sets are still generic. Headtracking is supported.)</p><p class="P3"> </p><p class="P3">II. I see currently two ways to derive or &lt; synthesize &gt; BRIRs from personalized HRTF data sets:</p><p class="P3"> </p><p class="P3">- You could calculate BRIRs from simulated (synthetic) room responses of virtual loudspeakers, convoluted over (personalized) HRTF filters.</p><p class="P3">OR</p><p class="P3">- You could &lt; synthesize &gt; BRIRs, derived from measured RIRs and personalized HRTF data sets.</p><p class="P3"> </p><p class="P3">Both methods have been discussed by OHTI team members (Bo-Erik Sandholm and me) with outside contacts.</p><p class="P3"> </p><p class="P3">In the end our method will separate RIRs and HRTFs, giving freedom to substitute and individualize both elements. Methods to upsample FOA RIRs (measured or calculated) to higher Ambisonics orders are known, for example SIRR and (as newer method) SDM.</p><p class="P3"> </p><p class="P3">Our preliminary impression is that BRIR synthesis is a task for which there is no trivial solution. (A few related papers are available online, but IMHO there is probably no “production-ready” and complete solution available yet - at least not in open form. We plan to reproduce channel-based audio sources with help of synthesized BRIRs a bit later - and are already working on some implementation.)</p><p class="P3"> </p><p class="P3">The case of audio objects needs a separate discussion: In VR and good audio game engines you normally would calculate the environmental response of audio objects in different acoustics (simulating many details such as occlusion, surface properties etc.), and filter over (personalized) HRTFs. </p><p class="P3">In other cases you might simply pan audio objects into a speaker “bed” or into some hoa soundfield....</p><p class="P3"> </p><p class="P3"> </p><p class="P3">III. The combination of headtracked and personalized audio &lt; including the room response &gt; is also known as a so-called “Binaural Room Scanning” (BRS) system. Such a system can basically reproduce any  surround or 3DA loudspeaker system via headphones. (Or you reproduce sound fields and audio objects in more direct ways, without using a virtual speaker approach. But the latter will probably be required for channel-based content.)</p><p class="P3"> </p><p class="P3">You could suspect that time has come to implement and democratize BRS solutions with the means of certain new but widely available techniques, such as “VR Audio” methods and modern dmotion sensors, measuring drift-free orientation.</p><p class="P3"> </p><p class="P3">A full  and complete “BRS” profile could include some more features, such as room synthesis from RIRs and personalized HRTFs (or just measured BRIRs), generic headphone correction, and  even &lt; personalized &gt; equalization of the (individually measured) headphone response. (The headphone - head combination must be seen as a “system”  introducing certain feedback and resonance effects, so this is not just a headphone quality issue.)</p><p class="P3"/><p class="P3">The general aim of a any such solution is of course to deliver a widely available and cheap platform to reproduce &lt; any &gt;  surround, 3D audio and immersive audio source over motion-tracked headphones. Which could bring the breakthrough for surround music recordings - surround here meant in the widest sense.</p><p class="P3">A motion-tracked headphone can be realized with the help of a tracking module, connected to some binaural player installed on some host system. (PC, smartphone, tablet, home audio server etc.)</p><p class="P3"> </p><p class="P3">All presented elements (headtracking, hrir personalisation etc.) can be applied for audio-only content and music, but also for TV/cinema and game audio. (AV applications)</p><p class="P3">Support of headtracking is obviously a requirement for VR audio. HRIR  personalisation is some further and optional feature to improve the quality of audio reproduction.</p><p class="P3"> </p><p class="P3">The missing part to standardize a BRS system is a credible and hopefully open way to derive BRIRs from HRTFs and RIRs. (HRTF matching and personalization services are started to be provided by companies such as 3D Sound Labs, IDA Audio etc. Loudspeaker RIRs can be measured in good reference rooms. There are credible ways to sharpen room impulse responses, such as the SIRR etc.)</p><p class="P3"> </p><p class="P3">IV.  Possible extension of OHTI to 6DoF</p><p class="P3"> </p><p class="P3">There are some credible ways to extend our motion sensor API and sensor hardware/software to 6DoF.</p><p class="P3">You would fusion some IMU sensor data and video data - taken by one or two cameras - via libraries such as Google WordSense/Microsoft WorldScale, or especially Google ARCore, providing (open) support of 3DoF+/6DoF sensor fusion.</p><p class="P3">The computations needed for  sensor fusion would be done either by a host computer/device, or by some onboard processor. If there is no USB connection between OHTI (6DoF) sensor and host system, the onboard-variant might be the preferable or even the only practical way.</p><p class="P3"> </p><p class="P3">Such an implementation is meant to be a lightweight but full 6DoF (non-HMD...) motion and head tracker for audio applications, not requiring people to wear a “full”  HMD or AR glass to serve as tracking device for audio-only and AV applications.</p><p class="P3"> </p><p class="P3"> </p><p class="P3">————————————————————</p><p class="P3"> </p><p class="P3"> </p><p class="P3">The OHTI project’s aim is to found a basis which could bring surround music, 3D audio and immersive audio to many people. A standardized binaural reproduction system supporting headtracking for channel-based and sound field audio sources could be a relatively cheap and realistic way to achieve this aim.</p><p class="P3">OHTI currently supports ambisonics up to third order, in AmbiX format. The provided decoder implementations are of course supporting our headtracker reference.  HRTF personalization is already supported. (Compared to Google Omnitone we add headtracker support for audio-only/music and AV content. The personalisation feature could be copied - or “implemented” - by Google for VR applications, in which case we would be thankful if Google would attribute the origin of this improvement idea to us. We took Omnitone from Google, modified and gave it finally back... ;-)</p><p class="P3"> </p><p class="P3">The OHTI beta version will add support for 2.0/stereo and 5.1 audio sources - including music recordings, TV/cinema audio and game audio. These are still and will be for some time to come the two most frequent channel-based audio formats, by a big margin. The binauralization method will be BRIR-based, optionally supporting &lt; personalized &gt; BRIR filters.</p><p class="P3">(Support of other “C” formats such as 7.1, Auro-3D and 7.1.4 could be added later.)</p><p class="P3"> </p><p class="P3">Because next to everybody is (at least sometimes) listening to music, you could think that especially the proposed audio-only/music applications of OHTI are potentially quite relevant, at least from a cultural perspective. (Largely improved binural representation of all music content/recordings.)</p><p class="P3">Unfortunately this is not the “industry” view, but it is certainly my one.</p><p class="P3">(However, I would like to remember that Apple has seen the commercial significance of “digital” music quite early, whereas most of the “music industry” didn’t. I refer of course to the success of the iPod and iTunes. From today’s perspective it could well be that there are more opportunities in surround sound, 3D Audio and immersive audio than certain people could imagine. Even if these commercial opportunities would not exist, the quest for better quality in music/audio recording and reproduction is a valid and sufficient aim by itself.)</p><p class="P3"> </p><p class="P3">Currently OHTI modifies Google Omnitone and JSAmbisonics to reproduce headtracked ambisonics sources.  If a person already owns a personalized SOFA file we are also able to support matched or personalized HRTF data sets, definitively improving in this area on Omnitone and Google VR Audio. (JSAmbisonics is in this aspect clearly ahead of Omnitone, providing personalization via library functions. In this case we just had to add the open headtracker, which is steering the decoding process.)</p><p class="P3"> </p><p class="P3">I have been quite a bit involved in 5.1 initiatives, about 15 years ago. I believe that there is currently a new window of opportunity to present the advantages of surround music /3D Audio to some wider audience, using available “VR Audio” technologies to improve headphone reproduction of any surround standard - and even stereo. (People preferring loudspeaker installations might have a look to Mpeg 3DA, which offers flexible adaptation strategies to match the source format to the available destination speaker layout. )</p><p class="P3"> </p><p class="P3">In Annex A I will present the single technical measures  (i.e. six steps) involved in OHTI, at “higher resolution”. (This is some additional technical information, if needed.)</p><p class="P3"> </p><p class="P3"> </p><p class="P3">During the lifetime of the project we have consulted quite a few people, receiving some very valuable feedback.</p><p class="P3">Just to mention a few and of course to say “thank you”, I would like to name Marc Lavallée (advice on hardware and software development), Rodrigo Ventura (orientation standards, from the perspective of a robotics expert), Antti Vanne (hrtf personalization), Archontis Politis (JSAmbisonics, upsampling strategies for RIRs), Aaron Heller (information about spherical hrtfs and ambisonics decoder optimization), and Schuyler Quackenbush (advanced information about Mpeg 3DA and Mpeg-I Audio; ultimately also convincing me and probably “us” from the OHTI project that 3DoF+/6DoF motion-tracked audio could be needed and implemented later.)</p><p class="P3"> </p><p class="P3">The OHTI initiative is an ongoing project, both as an open hw/sw project and as an API based tracking standard.</p><p class="P3"> </p><p class="P3"> </p><p class="P3">Stefan Schreiber                              Lisbon, 15th of May 2018</p><p class="P3"> </p><p class="P3">-------------------------------------------------</p><p class="P3"> </p><h1 class="P5"><a id="a__Annex_A"><span/></a>Annex A</h1><p class="P3"> </p><p class="P3">I will now rewrite the OHTI project’s architecture in an order based on the involved technical steps, proposing about six different elements (currently) to consider.</p><p class="P3"> </p><p class="P3">1. Open Headtracking Initiative Reference Hard- and Software</p><p class="P3"> </p><p class="P3">An open headtracker reference hardware and software  has been established, which easily can be changed, improved and manufactured.</p><p class="P3"> </p><p class="P3">An Internet presence is currently located at https://github.com/bossesand/ohti .</p><p class="P3"> </p><p class="P3">The current OHTI headtracker sw currently supports "only" headtracked ambisonics playback (orientational teacking), and (SOFA-based) hrtf personalisation.</p><p class="P3">Applying camera-based hardware extensions and certain software (Google WorldSense, especially open-sourced ARCore), a 6DoF extension looks to be doable.</p><p class="P3"> </p><p class="P3">Said this it is obvious that a companies such as Qualcomm and probably also Google could realize such a “lightweight”  6DoF sensor within the shortest time: You just would have to "strip down" the Qualcomm Snapdragon 835/845 VR reference headsets, which already support 6DoF inside-out tracking. Google is the provider of the open ARCore libraries.</p><p class="P3">(So this is an abstract “proof of concept”, comparing to state of the art mobile VR solutions.)</p><p class="P3"> </p><p class="P3"> </p><p class="P3">2. The OHTI team and project will define and support some &lt; Headtracking/tracking APIs &gt;.</p><p class="P3"> </p><p class="P3">The main function will certainly be some "get_orientation(W, X, Y, Z)" function, W to Z being quaternion (real) numbers between -1 and 1.</p><p class="P3"> </p><p class="P3">Get_orientation(yaw, pitch, roll) is also supported, and is according to me equivalent - as long as you don't have to "write" i.e. &lt; correct &gt; individual axis orientations. (In this case so-called gimbal locks both will happen and matter.)</p><p class="P3"> </p><p class="P3">Functions for directional reset and (generalized) calibration will also be supported. The rest is currently open for discussion.</p><p class="P3"> </p><p class="P3">6DoF: In an abstract way, a get_pose(...) function would add three Cartesian coordinates to both functions above. So 6DoF is a simple extension of 3DoF, seen from a standardization perspective...</p><p class="P3"> </p><p class="P3">Other functions of a 6DoF API would be very similar to the 3 DoF case, as well.</p><p class="P3"> </p><p class="P3">(So: The problem of 6DoF support is all about implementation, not API standardization. :-)</p><p class="P3"> </p><p class="P3"> </p><p class="P3">3. Personalisation</p><p class="P3"> </p><p class="P3">Our aim is to add support of (individualized or matched) HRTF &lt; and &gt; BRIR data sets/files to OHTI playback software, improving perceived audio quality during binaural audio reproduction.</p><p class="P3"> </p><p class="P3">The proposed hrir format to be directly supported within OHTI decoders could be the AES-normed SOFA/AES-69 format. (OHTI already supports AES-69, using still some intermediate HRIR formats to adapt to Omnitone and JSAmbisonics.)</p><p class="P3"> </p><p class="P3">RIRs would supposedly be presented in Ambisonics B format (AmbiX; to avoid confusion if providing RIRs in HOA later.)</p><p class="P3"> </p><p class="P3"> </p><p class="P3">4. BRIR calculation/synthesis</p><p class="P3"> </p><p class="P3">Whereas (3) is certainly within reach of some current commercial solutions and already implemented in the OHTI reference sw, channel-based content reproduced via headphones would rather need a solution based on BRIRs. (As in any existing brs system, for example the A8/16 Realiser manufactured and offered by Smyth Research.)</p><p class="P3"> </p><p class="P3">Calculation of brirs  &lt; could &gt; involve synthetic room models/acoustical simulation. (for example involving audio object/loudspeaker room responses, convolution of source signals, rotation and filtering via hrtfs - in a simplified description.)</p><p class="P3"> </p><p class="P3">An alternative would be to synthesize brirs from measured RIRs and (generic or personal) HRTFs.</p><p class="P3"> </p><p class="P3"> </p><p class="P3">5. Immersive Audio Profile (OHTI)</p><p class="P3"> </p><p class="P3">Whereas proposal (3) would improve on current Omnitone and Google VR Audio, (1) to (4) or (2) to (4) could define a solid basis for an audio-only (+ music) and AV profile of OHTI.</p><p class="P3"> </p><p class="P3"> </p><p class="P3">6. Management of Sofa files/ hrtfs/ brirs</p><p class="P3"> </p><p class="P3">Especially in the OHTI case an Ambisonics player (Omnitone, JSAmbisonics) might be directly downloaded from and into a browser.</p><p class="P3">The OHTI sw middleware connects to the browser via a quite common technique called WebSockets. (In-browser programs are very limited in direct system access, for security reasons.)</p><p class="P3">&lt; The downloaded JS/HTML5 player won't be personalized. &gt; (important observation)</p><p class="P3">So the management, loading and handling of the personal SOFA/AES-69 data (hrirs) should possibly be supported from and within the OHTI runtime environment.</p><p class="P3"> </p><p class="P3">----------------------------------------------------------</p></body></html>